{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It is commonly used to transform a high-dimensional dataset into a lower-dimensional representation while preserving the most important information.\n",
    "\n",
    "PCA works by finding the directions (principal components) in the data that explain the maximum variance. These principal components are orthogonal to each other, meaning they are uncorrelated. The first principal component captures the most variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "Applications of PCA include:\n",
    "\n",
    "- Dimensionality reduction: PCA can be used to reduce the number of features in a dataset, making it easier to visualize and analyze. It can help in dealing with the curse of dimensionality and improve the performance of machine learning models.\n",
    "\n",
    "- Data visualization: PCA can be used to visualize high-dimensional data in a lower-dimensional space. By projecting the data onto the principal components, we can create scatter plots or other visualizations that capture the most important patterns in the data.\n",
    "\n",
    "- Noise reduction: PCA can be used to remove noise from data by eliminating the principal components with low variance. This can be useful in denoising signals or images.\n",
    "\n",
    "- Feature extraction: PCA can be used to extract the most important features from a dataset. By selecting a subset of the principal components, we can create a new feature representation that captures the essential information in the data.\n",
    "\n",
    "- Anomaly detection: PCA can be used to detect anomalies in data by comparing the reconstruction error of a data point with the original data. Points with high reconstruction error are likely to be outliers or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 22:13:23 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface wlo1)\n",
      "24/03/27 22:13:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/27 22:13:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"pca\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
